{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TOC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "* Logistic Regression 事实上是广义线性回归，将普通线性回归（OLS）的结果，对应到二元分类里面\n",
    "\n",
    "### 模型\n",
    "* 考虑到函数性质（可微），我们选取sigmoid函数将OLS结果转化成｛0，1｝变量。sigmoid函数图像（[代码见附录](#附录1)）如下所示:\n",
    "$$y=\\frac{1}{1+e^{-x}}$$\n",
    "<div align=\"center\"><img src=\"ML_pic\\sigmoid.png\"></div>\n",
    "* 从而，我们令\n",
    "$$样本方程： h_i=\\frac{1}{1+e^{X_{i}^{T}\\beta}} \\Rightarrow \\ln\\frac{h_i}{1-h_i} = X_{i}^{T}\\beta$$ \n",
    "$$总体方程： H=\\frac{1}{1+e^{X\\beta}}$$  \n",
    "其中：  \n",
    "$$X = \\begin{bmatrix}\n",
    "      1      & x_{1}^{1} & x_{1}^{2} & \\cdots & x_{1}^{d} \\\\\n",
    "      \\vdots & \\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n",
    "      1      & x_{n}^{1} & x_{n}^{2} & \\cdots & x_{n}^{d} \n",
    "      \\end{bmatrix} \\qquad\n",
    "  X_i =  \\begin{bmatrix}\n",
    "      1      & x_{i}^{1} & x_{i}^{2} & \\cdots & x_{i}^{d} \n",
    "      \\end{bmatrix}^T\n",
    "$$\n",
    "$$\n",
    "  Y =  \\begin{bmatrix}\n",
    "      y_{1}     & y_{2} & y_{3} & \\cdots & y_{n}\n",
    "      \\end{bmatrix}^T \\qquad\n",
    "  \\beta =  \\begin{bmatrix}\n",
    "      \\beta_{1}     & \\beta_{2} & \\beta_{3} & \\cdots & \\beta_{n}\n",
    "      \\end{bmatrix}^T\n",
    "$$\n",
    "\n",
    "\n",
    "**如果我们把$h_i$看成是第$i$个样本分类为1的概率的话，这个模型会得到很好的理论解释。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法\n",
    "* 损失函数（Loss Function）：  \n",
    "    应用MLE，我们有似然函数：\n",
    "    $$\n",
    "    L(\\beta) = \\prod_{i=1}^{n}h_{i}^{y_i}(1-h_i)^{1-y_i}\n",
    "    $$\n",
    "    因此，有损失函数：\n",
    "    $$\n",
    "    l(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n}[{y_i}\\ln(h_i) + {(1-y_i)}\\ln(1-h_i)]\n",
    "    $$\n",
    "    从而，我们的问题变成了：\n",
    "    $$\n",
    "    \\hat{\\beta} = arg\\min_{\\beta}l(\\beta)\n",
    "    $$\n",
    "\n",
    "* 梯度下降法（Gradient Descend）:  \n",
    "    1. 梯度（Gradient）  \n",
    "       简单来说，梯度是函数导数的**反方向**。也就是说，梯度是一个**向量**。\n",
    "    2. Idea  \n",
    "       GD是求解***凸优化问题***的一种方法。比如函数$y=x^2$,给定任一点$x_0$,从这点出发，沿着梯度方向走，随着走的步数越来越多，其对应的函数值就越接近其最值。如图所示（[代码见附录](#附录1)）：  <div align=\"center\"><img src=\"ML_pic\\GD_pic.png\"></div>\n",
    "    3. 算法实现  \n",
    "       ① 任意$x_0$，计算梯度$d_0 = -\\frac{\\partial f}{\\partial x}|_{x=x_0}$  \n",
    "       ② 选择步长（学习率）$\\alpha$，更新公式为$x_1 = x_0 + \\alpha d_0$  \n",
    "       ③ 以此类推，可以通过判定阀值$\\epsilon$，要求$\\mid f(x_{k+1})-f(x_k) \\mid \\le \\epsilon$；\n",
    "          或者，选择设定最大迭代次数$k$，来停止迭代。\n",
    "    4. 用GD求解Logistic Regression  \n",
    "       ① diff $l(\\beta)$ w.r.t. $\\beta$ :\n",
    "       $$\n",
    "       \\frac{\\partial l}{\\partial \\beta} = -\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{T}(y_i-h_i)\n",
    "       = -\\frac{1}{n}X^T(y-h)\n",
    "       $$  \n",
    "       ② update:$\\beta = \\beta + \\alpha \\frac{\\partial l}{\\partial \\beta}$\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现\n",
    "* 导入数据集  \n",
    "   #这个note的所有数据集都可以在我的[GitHub](https://github.com/ocsphylee/Training_dataSet.git)主页找到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\" load data from txt file\n",
    "    this function requires data structure to be [features, label]\n",
    "\n",
    "    input:  path(str): the path of data file\n",
    "    output: label(mat): an n*1 matrix of label\n",
    "            features(mat): a n*(d+1) matrix of features\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            lines.append(line.split())\n",
    "    raw_data = np.array(lines, dtype=float)\n",
    "\n",
    "    n = raw_data.shape[0]\n",
    "    label = raw_data[:, -1].reshape((n, 1))\n",
    "    features = raw_data.copy()\n",
    "    features[:, -1] = 1\n",
    "\n",
    "    return np.mat(label), np.mat(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义sigmoid函数，并实现梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logit_gd(features, label, max_cycle, step):\n",
    "    \"\"\"train Logistic model with Gradient Descend\n",
    "    \n",
    "    input:  features(mat): a n*(d+1) matrix of features\n",
    "            label(mat): an n*1 matrix of label\n",
    "            max_cycle(int): maximum iteration times\n",
    "            step(float): learning ratio\n",
    "    output: beta(mat): a (d+1)*1 matrix of parameters\n",
    "    \"\"\"\n",
    "    n, d = features.shape\n",
    "    # initialize beta\n",
    "    beta = np.ones((d,1))\n",
    "\n",
    "    while max_cycle:\n",
    "        max_cycle -= 1\n",
    "        # calculate the gradient\n",
    "        err = label - sig(features * beta)\n",
    "        gd = features.T * err\n",
    "        \n",
    "        # track the approximate error (not necessary)\n",
    "        if max_cycle % 100 == 0:\n",
    "            error = np.sum(err) /n\n",
    "            print(\"error: {} {}\".format(max_cycle, error))\n",
    "            \n",
    "        # update beta\n",
    "        beta += step * gd\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 计算预测值和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(features, beta):\n",
    "    \"\"\"predict label uses trained model\n",
    "    input:  features(mat): a n*(d+1) matrix of features\n",
    "            beta(mat): a (d+1)*1 matrix of parameters\n",
    "    output: prediction(mat): a n * 1 predicted value of label\n",
    "    \"\"\"\n",
    "    h = sig(features * beta)\n",
    "    n = features.shape[0]\n",
    "    predict_label = []\n",
    "    for i in range(n):\n",
    "        if h[i,0]>0.5:\n",
    "            predict_label.append(1)\n",
    "            continue\n",
    "        predict_label.append(0)\n",
    "    prediction = np.array(predict_label).reshape((n,1))\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def get_accuracy(label, prediction):\n",
    "    \"\"\"calculate the accuracy of the model\n",
    "    input:  label(mat): an n*1 matrix of label\n",
    "            prediction(mat): a n * 1 predicted value of label\n",
    "    output: acc(float): accuracy of the model\n",
    "    \"\"\"\n",
    "    n = label.shape[0]\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        if label[i,0] == prediction[i,0]:\n",
    "            result += 1\n",
    "    acc = result / n\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    label, features = load_data(\"data/1.logit_data.txt\")\n",
    "    beta = logit_gd(features, label, 1000, 0.01)\n",
    "    prediction = get_predict(features,beta)\n",
    "    accuracy = get_accuracy(label,prediction)\n",
    "    print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 附录1\n",
    "* sigmoid 图像绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sig(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.ones((100,))\n",
    "plt.plot(x, sig(x))\n",
    "plt.plot(x, y, c='grey', linestyle='--')\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)  # 去掉上边框\n",
    "ax.spines['right'].set_visible(False)  # 去掉右边框\n",
    "ax.spines['left'].set_position(('data', 0))  # 移动左坐标轴到数据为0的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GD图示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "step = 10\n",
    "x0 = 8\n",
    "while step>0:\n",
    "    step -= 1 \n",
    "    x1 = x0 - 2 * x0*0.245\n",
    "    plt.scatter(x0,f(x0),c=\"r\",s=10)\n",
    "    plt.plot([x0,x1],[f(x0),f(x1)],\"r\")\n",
    "    x0 = x1\n",
    "plt.plot(x,f(x))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)  # 去掉上边框\n",
    "ax.spines['right'].set_visible(False)  # 去掉右边框\n",
    "ax.spines['left'].set_position(('data', 0))  # 移动左坐标轴到数据为0的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataServer",
   "language": "python",
   "name": "dataserver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
